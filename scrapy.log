2026-01-18 04:42:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-01-18 04:42:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-01-18 04:42:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-01-18 04:42:03 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline']
2026-01-18 04:42:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-18 04:42:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-18 04:42:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-18 04:42:03 [scrapy.core.engine] INFO: Spider opened
2026-01-18 04:42:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-01-18 04:42:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-01-18 04:42:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-01-18 04:42:03 [simple] INFO: clashmeta start
2026-01-18 04:42:03 [simple] INFO: ndnode start
2026-01-18 04:42:03 [simple] INFO: nodev2ray start
2026-01-18 04:42:03 [simple] INFO: nodefree start
2026-01-18 04:42:03 [simple] INFO: v2rayshare start
2026-01-18 04:42:03 [simple] INFO: wenode start
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-15-free-node-subscribe.htm on 2026-01-15
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-15-free-node-subscribe.htm on 2026-01-15
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-14-today-clash-meta-node.htm on 2026-01-14
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-14-today-clash-meta-node.htm on 2026-01-14
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-12-free-high-speed-nodes.htm on 2026-01-12
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-12-free-high-speed-nodes.htm on 2026-01-12
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-11-clash-meta-github.htm on 2026-01-11
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-11-clash-meta-github.htm on 2026-01-11
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-10-clash-meta-node-github.htm on 2026-01-10
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-10-clash-meta-node-github.htm on 2026-01-10
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-9-free-high-speed-nodes.htm on 2026-01-09
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-9-free-high-speed-nodes.htm on 2026-01-09
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-8-free-subscribe-node.htm on 2026-01-08
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-8-free-subscribe-node.htm on 2026-01-08
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-7-clash-meta-node.htm on 2026-01-07
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-7-clash-meta-node.htm on 2026-01-07
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-6-clash-meta-github.htm on 2026-01-06
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-6-clash-meta-github.htm on 2026-01-06
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-5-free-node-subscribe.htm on 2026-01-05
2026-01-18 04:42:03 [simple] INFO: clashmeta found /free-nodes/2026-1-5-free-node-subscribe.htm on 2026-01-05
2026-01-18 04:42:03 [simple] INFO: clashmeta needs update, accessing https://clash-meta.github.io/free-nodes/2026-1-15-free-node-subscribe.htm
2026-01-18 04:42:03 [simple] INFO: clashmeta needs update, accessing https://clash-meta.github.io/free-nodes/2026-1-15-free-node-subscribe.htm
2026-01-18 04:42:03 [simple] INFO: clashmeta needs update, accessing https://clash-meta.github.io/free-nodes/2026-1-14-today-clash-meta-node.htm
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5305.html on 2026-01-18
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5306.html on 2026-01-17
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5307.html on 2026-01-16
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5308.html on 2026-01-15
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5309.html on 2026-01-14
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5310.html on 2026-01-13
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5311.html on 2026-01-12
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5312.html on 2026-01-11
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5313.html on 2026-01-10
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5314.html on 2026-01-09
2026-01-18 04:42:03 [simple] INFO: v2rayshare needs update, accessing https://v2rayshare.net/p/5305.html
2026-01-18 04:42:03 [simple] INFO: v2rayshare needs update, accessing https://v2rayshare.net/p/5306.html
2026-01-18 04:42:03 [simple] INFO: v2rayshare is up to date, exiting
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3535.html on 2026-01-17
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3536.html on 2026-01-16
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3537.html on 2026-01-15
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3538.html on 2026-01-14
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3539.html on 2026-01-13
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3540.html on 2026-01-12
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3541.html on 2026-01-11
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3542.html on 2026-01-10
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3543.html on 2026-01-09
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.cc/a/3544.html on 2026-01-08
2026-01-18 04:42:03 [simple] INFO: wenode needs update, accessing https://oneclash.cc/a/3535.html
2026-01-18 04:42:03 [simple] INFO: wenode is up to date, exiting
2026-01-18 04:42:03 [simple] INFO: wenode is up to date, exiting
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.githubrowcontent.com/2026/01/20260118.txt
2026-01-18 04:42:03 [simple] INFO: v2rayshare found https://v2rayshare.githubrowcontent.com/2026/01/20260118.yaml
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-18-node-share.htm on 2026-01-18
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-17-free-ssr-subscribe.htm on 2026-01-17
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-16-free-node-subscribe.htm on 2026-01-16
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-15-clash-v2ray-ss-ssr.htm on 2026-01-15
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-14-free-subscribe-node.htm on 2026-01-14
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-13-clash-v2ray-ss-ssr.htm on 2026-01-13
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-12-free-high-speed-nodes.htm on 2026-01-12
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-11-free-node-subscribe-links.htm on 2026-01-11
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-10-free-clash-subscribe.htm on 2026-01-10
2026-01-18 04:42:03 [simple] INFO: nodev2ray found /free-node/2026-1-9-free-subscribe-node.htm on 2026-01-09
2026-01-18 04:42:03 [simple] INFO: nodev2ray needs update, accessing https://nodev2ray.com/free-node/2026-1-18-node-share.htm
2026-01-18 04:42:03 [simple] INFO: nodev2ray is up to date, exiting
2026-01-18 04:42:03 [simple] INFO: nodev2ray is up to date, exiting
2026-01-18 04:42:03 [simple] WARNING: wenode could not parse https://oneclash.githubrowcontent.com/2026/0..."/, skipping
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.githubrowcontent.com/2026/01/20260117.txt
2026-01-18 04:42:03 [simple] INFO: wenode found https://oneclash.githubrowcontent.com/2026/01/20260117.yaml
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3507.html on 2026-01-18
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3505.html on 2026-01-17
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3503.html on 2026-01-16
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3501.html on 2026-01-15
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3499.html on 2026-01-14
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3497.html on 2026-01-13
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3495.html on 2026-01-12
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3493.html on 2026-01-11
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3492.html on 2026-01-10
2026-01-18 04:42:04 [simple] INFO: ndnode found https://www.naidounode.com/n/3490.html on 2026-01-09
2026-01-18 04:42:04 [simple] INFO: ndnode needs update, accessing https://www.naidounode.com/n/3507.html
2026-01-18 04:42:04 [simple] INFO: ndnode is up to date, exiting
2026-01-18 04:42:04 [simple] INFO: ndnode is up to date, exiting
2026-01-18 04:42:04 [simple] INFO: v2rayshare found https://v2rayshare.githubrowcontent.com/2026/01/20260117.txt
2026-01-18 04:42:04 [simple] INFO: v2rayshare found https://v2rayshare.githubrowcontent.com/2026/01/20260117.yaml
2026-01-18 04:42:04 [simple] INFO: ndnode found http://naidounode.cczzuu.top/node/20260118-v2ray.txt
2026-01-18 04:42:04 [simple] INFO: ndnode found http://naidounode.cczzuu.top/node/20260118-clash.yaml
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3903.html on 2026-01-17
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3904.html on 2026-01-16
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3905.html on 2026-01-15
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3906.html on 2026-01-14
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3907.html on 2026-01-13
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3908.html on 2026-01-12
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3909.html on 2026-01-11
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3910.html on 2026-01-10
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3911.html on 2026-01-09
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.me/p/3912.html on 2026-01-08
2026-01-18 04:42:04 [simple] INFO: nodefree needs update, accessing https://nodefree.me/p/3903.html
2026-01-18 04:42:04 [simple] INFO: nodefree is up to date, exiting
2026-01-18 04:42:04 [simple] INFO: nodefree is up to date, exiting
2026-01-18 04:42:04 [simple] INFO: Pipeline got item wenode.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline processed wenode.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline got item wenode.yaml
2026-01-18 04:42:04 [simple] INFO: Pipeline processed wenode.yaml
2026-01-18 04:42:04 [simple] INFO: nodev2ray found https://node.nodev2ray.com/uploads/2026/01/0-20260118.txt
2026-01-18 04:42:04 [simple] INFO: nodev2ray found https://node.nodev2ray.com/uploads/2026/01/4-20260118.txt
2026-01-18 04:42:04 [simple] INFO: nodev2ray found https://node.nodev2ray.com/uploads/2026/01/2-20260118.yaml
2026-01-18 04:42:04 [simple] INFO: nodev2ray found https://node.nodev2ray.com/uploads/2026/01/4-20260118.yaml
2026-01-18 04:42:04 [simple] WARNING: nodev2ray could not parse https://node.nodev2ray.com/uploads/2026/01/20260118.json, skipping
2026-01-18 04:42:04 [simple] INFO: Pipeline got item v2rayshare.yaml
2026-01-18 04:42:04 [simple] INFO: Pipeline processed v2rayshare.yaml
2026-01-18 04:42:04 [simple] INFO: Pipeline got item v2rayshare.yaml
2026-01-18 04:42:04 [simple] INFO: Pipeline processed v2rayshare.yaml
2026-01-18 04:42:04 [simple] INFO: Pipeline got item v2rayshare.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline processed v2rayshare.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline got item v2rayshare.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline processed v2rayshare.txt
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.githubrowcontent.com/2026/01/20260117.txt
2026-01-18 04:42:04 [simple] INFO: nodefree found https://nodefree.githubrowcontent.com/2026/01/20260117.yaml
2026-01-18 04:42:04 [simple] INFO: Pipeline got item nodev2ray.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline processed nodev2ray.txt
2026-01-18 04:42:04 [simple] INFO: Pipeline got item nodev2ray.yaml
2026-01-18 04:42:05 [simple] INFO: Pipeline processed nodev2ray.yaml
2026-01-18 04:42:05 [simple] INFO: Pipeline got item nodev2ray.yaml
2026-01-18 04:42:05 [simple] INFO: Pipeline processed nodev2ray.yaml
2026-01-18 04:42:05 [simple] INFO: Pipeline got item nodev2ray.txt
2026-01-18 04:42:05 [simple] INFO: Pipeline processed nodev2ray.txt
2026-01-18 04:42:05 [simple] INFO: Pipeline got item ndnode.txt
2026-01-18 04:42:05 [simple] INFO: Pipeline processed ndnode.txt
2026-01-18 04:42:05 [simple] INFO: Pipeline got item nodefree.txt
2026-01-18 04:42:05 [simple] INFO: Pipeline processed nodefree.txt
2026-01-18 04:42:05 [simple] INFO: Pipeline got item nodefree.yaml
2026-01-18 04:42:05 [simple] INFO: Pipeline processed nodefree.yaml
2026-01-18 04:42:05 [simple] INFO: Pipeline got item ndnode.yaml
2026-01-18 04:42:06 [simple] INFO: Pipeline processed ndnode.yaml
2026-01-18 04:42:06 [scrapy.core.engine] INFO: Closing spider (finished)
2026-01-18 04:42:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 11392,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 1018705,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 28,
 'downloader/response_status_count/301': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 2.930437,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 1, 18, 4, 42, 6, 151532, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 662499,
 'httpcompression/response_count': 21,
 'item_scraped_count': 14,
 'items_per_minute': 420.0,
 'log_count/INFO': 139,
 'log_count/WARNING': 2,
 'memusage/max': 177090560,
 'memusage/startup': 177090560,
 'request_depth_max': 2,
 'response_received_count': 28,
 'responses_per_minute': 840.0,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2026, 1, 18, 4, 42, 3, 221095, tzinfo=datetime.timezone.utc)}
2026-01-18 04:42:06 [scrapy.core.engine] INFO: Spider closed (finished)
2026-01-18 04:42:13 [scrapy.addons] INFO: Enabled addons:
[]
2026-01-18 04:42:13 [scrapy.extensions.telnet] INFO: Telnet Password: 6e2c9aa98bfe5a72
2026-01-18 04:42:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logcount.LogCount',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2026-01-18 04:42:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'NodeScrapy',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'NodeScrapy.spiders',
 'SPIDER_MODULES': ['NodeScrapy.spiders']}
2026-01-18 04:42:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-01-18 04:42:13 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-01-18 04:42:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-01-18 04:42:13 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline']
2026-01-18 04:42:13 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-18 04:42:13 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-18 04:42:13 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-18 04:42:13 [scrapy.core.engine] INFO: Spider opened
2026-01-18 04:42:13 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider (inherited by NodeScrapy.spiders.DecryptSpider.DecryptSpider) defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-01-18 04:42:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-01-18 04:42:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-01-18 04:42:13 [decrypt] INFO: yudou66 start
2026-01-18 04:42:13 [decrypt] INFO: blues start
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261184kchatgpt4k8kclashv2ray.html on 2026-01-18
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261174kchatgpt4k8kclashv2ray.html on 2026-01-17
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261164kchatgpt4k8kclashv2ray.html on 2026-01-16
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261154kchatgpt4k8kclashv2ray.html on 2026-01-15
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261144kchatgpt4k8kclashv2ray.html on 2026-01-14
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261134kchatgpt4k8kclashv2ray.html on 2026-01-13
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261124kchatgpt4k8kclashv2ray.html on 2026-01-12
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261114kchatgpt4k8kclashv2ray.html on 2026-01-11
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/20261104kchatgpt4k8kclashv2ray.html on 2026-01-10
2026-01-18 04:42:13 [decrypt] INFO: blues found https://blues2022.blogspot.com/2026/01/2026194kchatgpt4k8kclashv2ray.html on 2026-01-09
2026-01-18 04:42:13 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2026/01/20261184kchatgpt4k8kclashv2ray.html
2026-01-18 04:42:13 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2026/01/20261174kchatgpt4k8kclashv2ray.html
2026-01-18 04:42:13 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2026/01/20261164kchatgpt4k8kclashv2ray.html
2026-01-18 04:42:16 [decrypt] WARNING: blues 6622 got 密碼錯誤
2026-01-18 04:42:17 [decrypt] WARNING: blues 0000 got 密碼錯誤
2026-01-18 04:42:18 [decrypt] WARNING: blues 0011 got 密碼錯誤
2026-01-18 04:42:20 [decrypt] WARNING: blues 0022 got 密碼錯誤
2026-01-18 04:42:21 [decrypt] WARNING: blues 0033 got 密碼錯誤
2026-01-18 04:42:22 [decrypt] WARNING: blues 0044 got 密碼錯誤
2026-01-18 04:42:23 [decrypt] WARNING: blues 0055 got 密碼錯誤
2026-01-18 04:42:26 [decrypt] INFO: blues saved new password 0066
2026-01-18 04:42:31 [decrypt] WARNING: blues 0066 got 密碼錯誤
2026-01-18 04:42:31 [decrypt] WARNING: blues 0000 got 密碼錯誤
2026-01-18 04:42:32 [decrypt] WARNING: blues 0011 got 密碼錯誤
2026-01-18 04:42:34 [decrypt] WARNING: blues 0022 got 密碼錯誤
2026-01-18 04:42:34 [decrypt] WARNING: blues 0033 got 密碼錯誤
2026-01-18 04:42:35 [decrypt] WARNING: blues 0044 got 密碼錯誤
2026-01-18 04:42:36 [decrypt] WARNING: blues 0055 got 密碼錯誤
2026-01-18 04:42:40 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yudou6677.top/> (failed 3 times): DNS lookup failed: no results for hostname lookup: www.yudou6677.top.
2026-01-18 04:42:40 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.yudou6677.top/>
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/engine.py", line 490, in _download
    result = yield self.downloader.fetch(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/internet/defer.py", line 1853, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/__init__.py", line 144, in fetch
    yield deferred_from_coro(
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/internet/defer.py", line 1257, in adapt
    extracted: _SelfResultT | Failure = result.result()
                                        ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py", line 158, in download_async
    result = await process_exception(ex)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py", line 150, in process_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py", line 153, in download_async
    result: Response | Request = await process_request(request)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py", line 97, in process_request
    return await download_func(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/__init__.py", line 202, in _enqueue_request
    return await maybe_deferred_to_future(d)  # fired in _wait_for_download()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/__init__.py", line 269, in _wait_for_download
    response = await self._download(slot, request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/__init__.py", line 239, in _download
    response: Response = await self.handlers.download_request_async(request)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/handlers/__init__.py", line 156, in download_request_async
    return await handler.download_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/handlers/http11.py", line 113, in download_request
    return await maybe_deferred_to_future(agent.download_request(request))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/internet/defer.py", line 1092, in _runCallbacks
    current.result = callback(  # type: ignore[misc]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/twisted/internet/endpoints.py", line 1091, in startConnectionAttempts
    raise error.DNSLookupError(
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: www.yudou6677.top.
2026-01-18 04:42:40 [scrapy.core.engine] INFO: Closing spider (finished)
2026-01-18 04:42:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 3320,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 166376,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 4,
 'downloader/response_status_count/302': 2,
 'elapsed_time_seconds': 27.347172,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 1, 18, 4, 42, 40, 584308, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 971560,
 'httpcompression/response_count': 4,
 'items_per_minute': 0.0,
 'log_count/ERROR': 2,
 'log_count/INFO': 19,
 'log_count/WARNING': 14,
 'memusage/max': 177246208,
 'memusage/startup': 177246208,
 'request_depth_max': 1,
 'response_received_count': 4,
 'responses_per_minute': 8.88888888888889,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 2,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2026, 1, 18, 4, 42, 13, 237136, tzinfo=datetime.timezone.utc)}
2026-01-18 04:42:40 [scrapy.core.engine] INFO: Spider closed (finished)
